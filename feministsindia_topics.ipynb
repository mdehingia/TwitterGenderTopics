{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import printable\n",
    "import re \n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Twitterdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[(data[\"tweetcreatedts\"]>=\"2020-03-24\")]\n",
    "data1[\"date\"]=pd.to_datetime(data1[\"tweetcreatedts\"], format='%Y-%m-%d %H:%M:%S')\n",
    "data1[\"date\"]=data1[\"date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing handle names\n",
    "data1[\"text_clean\"] = data1['text'].replace('@[^\\s]+', '', regex=True)\n",
    "#remove non-ASCII characters\n",
    "st = set(printable)\n",
    "data1[\"text_clean\"] = data1[\"text_clean\"].apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))\n",
    "#removing urls\n",
    "data1['text_clean'] = data1['text_clean'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "#converting to no caps\n",
    "data1[\"text_clean\"] = data1[\"text_clean\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_combined = ' '.join(list(data1['text_clean'].values))\n",
    "list_hashtag=[i for i in tweet_combined.split() if i.startswith('#')]\n",
    "final_ht=Counter(list_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 25 most tweeted hashtags\n",
    "df = pd.DataFrame.from_dict(final_ht, orient='index').reset_index()\n",
    "df.columns = ['hashtag', 'count']\n",
    "df=df[df.hashtag!='#']\n",
    "df=df.sort_values(by=['count'], ascending=False)\n",
    "hashtag=df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing non-alphabet characters except space - replacing non-alphabet with a space so that words don't get combined\n",
    "data1[\"text_clean\"] = data1['text_clean'].replace('[^a-zA-Z1-9 ]', ' ', regex=True)\n",
    "#getting ready to remove stop words\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "data1[\"text_clean\"] = data1[\"text_clean\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check = data1[data1.text_clean != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some cleaning of words\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        #print(sent)\n",
    "        sent = str.replace(sent, 'domesticviolence', 'domestic violence')\n",
    "        sent = str.replace(sent, 'domesticabuse', 'domestic abuse')\n",
    "        sent = str.replace(sent, 'intimatepartnerviolence', 'intimate partner violence')\n",
    "        sent = str.replace(sent, 'sexualviolence', 'sexual violence')\n",
    "        sent = str.replace(sent, 'genderbasedviolence', 'gender based violence')\n",
    "        sent = str.replace(sent, 'migrantworker', 'migrant worker')\n",
    "        sent = str.replace(sent, 'policebrutality', 'police brutality')\n",
    "        sent = str.replace(sent, 'casteviolence', 'caste violence')\n",
    "        sent = str.replace(sent, 'genderdata', 'gender data')\n",
    "        sent = str.replace(sent, 'domesticwork', 'domestic work')\n",
    "        sent = str.replace(sent, 'unpaidlabour', 'unpaid labour')\n",
    "        sent = str.replace(sent, 'unpaidlabor', 'unpaid labour')\n",
    "        sent = str.replace(sent, 'unpaidwork', 'unpaid work')\n",
    "        sent = str.replace(sent, 'labor', 'labour')\n",
    "        sent = str.replace(sent, 'coronaviruspandemic', 'coronavirus pandemic')\n",
    "        sent = str.replace(sent, 'coronaviruslockdown', 'coronavirus lockdown')\n",
    "        sent = str.replace(sent, 'coronalockdown', 'coronavirus lockdown')\n",
    "        sent = str.replace(sent, 'covidlockdown', 'covid lockdown')\n",
    "        sent = str.replace(sent, 'coronavirus', 'covid')\n",
    "        sent = str.replace(sent, 'corona', 'covid')\n",
    "        sent = str.replace(sent, 'covid19', 'covid')\n",
    "        sent = str.replace(sent, 'covid 19', 'covid')\n",
    "        sent = str.replace(sent, 'indian', 'india')\n",
    "        sent = str.replace(sent, 'indians', 'india')\n",
    "        sent = str.replace(sent, 'violenceagainstwomen', 'violence against women')\n",
    "        sent = str.replace(sent, 'pulitz', ' pulitz')\n",
    "        sent = str.replace(sent, 'kashmir', ' kashmir ')\n",
    "        sent = str.replace(sent, 'kashmiri', ' kashmiri ')\n",
    "        #print(sent)\n",
    "        yield(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text))\n",
    "\n",
    "\n",
    "def process_words(texts, stop_words=stop_words):\n",
    "    \"\"\"Form Bigrams, and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc), max_len=30) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts_out = []\n",
    "    #nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        sent=str(sent)\n",
    "        result=[]\n",
    "        for token in gensim.utils.simple_preprocess(sent, min_len=3, max_len=30):\n",
    "            result.append(lemmatize_stemming(token))\n",
    "        texts_out.append(result)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data2 = data_check.text_clean.values.tolist() \n",
    "\n",
    "c = Counter(word for x in data2 for word in x.split())\n",
    "texts = [' '.join(y for y in x.split() if c[y] > 1) for x in data2]\n",
    "data_words = list(sent_to_words(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=20, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(data_words)  \n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "id2word.filter_extremes(no_below=15, no_above=0.8)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ.update({'MALLET_HOME':r'C:\\\\mallet\\\\'}) \n",
    "os.environ['MALLET_HOME'] = 'C:\\\\mallet'\n",
    "mallet_path = r'C:\\\\mallet\\\\bin\\\\mallet.bat' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, iterations = 300, random_seed=12345)\n",
    "        model_list.append(model)\n",
    "        coherence_model_ldamallet = CoherenceModel(model=model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherence_model_ldamallet.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_ready, limit=40, start=10, step=1)\n",
    "# Show graph\n",
    "limit=40; start=10; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=18, id2word=id2word, iterations = 300, random_seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = CoherenceModel(model=ldamallet, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "print(coherence.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ldamallet.show_topics(formatted=False, num_words=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ldamallet, open( \"../lda.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamallet, corpus=corpus, texts=data_ready):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamallet[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            \n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamallet.show_topic(topic_num, num_words=30)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamallet=ldamallet, corpus=corpus, texts=data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df_dominant_topic[\"Dominant_Topic\"].to_numpy().tolist()\n",
    "b=df_dominant_topic[\"Topic_Perc_Contrib\"].to_numpy().tolist()\n",
    "c=df_dominant_topic[\"Keywords\"].to_numpy().tolist()\n",
    "d=data_check[\"text\"].to_numpy().tolist()\n",
    "e=data_check[\"likescount\"].to_numpy().tolist()\n",
    "f=data_check[\"retweetcount\"].to_numpy().tolist()\n",
    "g=data_check[\"username\"].to_numpy().tolist()\n",
    "h=data_check[\"date\"].to_numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(a,b,c,d,e,f, g, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = pd.DataFrame(data_tuples, columns=['Dominant_Topic','Topic_Perc_Contrib', 'Keywords', 'text','likescount','retweetcount', 'username', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"../topicsandtweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
